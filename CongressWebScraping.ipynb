{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. List of documents' IDs"
      ],
      "metadata": {
        "id": "mU9WjhakWG_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_id(url):\n",
        "    file_page = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
        "    doc_id = ','.join([a.text for a in file_page.select('span + .field li:nth-child(2) a')])\n",
        "    return doc_id\n",
        "\n",
        "document_list = []\n",
        "for page_result in range(1, 4):\n",
        "    link = f\"http://archivo.asambleanacional.gob.ec/index.php/search/advanced?page={page_result}&f=&so0=and&sq0=%222019-2021%22&sf0=title&so1=and&sq1=%22ACTAS+ASAMBLEA+NACIONAL%22&sf1=title&limit=100&sort=alphabetic\"\n",
        "    webpage_code = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
        "    file_urls = [f\"http://archivo.asambleanacional.gob.ec{a['href']}\" for a in webpage_code.select('.title a')]\n",
        "    doc_ids = [get_id(url) for url in file_urls]\n",
        "    document_list.extend(doc_ids)\n",
        "    print(f\"Page: {page_result}\")\n"
      ],
      "metadata": {
        "id": "eewF9_rlY_3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download documents\n"
      ],
      "metadata": {
        "id": "h7jWwt1uWyZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "\n",
        "def download_PDFs(url):\n",
        "    try:\n",
        "        page = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
        "        raw_list = [a['href'] for a in page.select('a') if re.search(r'\\.pdf$', a['href'])]\n",
        "\n",
        "        for file_url in raw_list:\n",
        "            filename = os.path.basename(file_url)\n",
        "            response = requests.get(file_url)\n",
        "            with open(filename, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! Download from {url} failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def get_links(pageNumber):\n",
        "    url = f\"http://archivo.asambleanacional.gob.ec/index.php/search/advanced?page={pageNumber}&f=&so0=and&sq0=%222019-2021%22&sf0=title&so1=and&sq1=%22ACTAS+ASAMBLEA+NACIONAL%22&sf1=title&limit=100&sort=alphabetic\"\n",
        "    webpage_code = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
        "    a_elements = webpage_code.select('.title a')\n",
        "    links = [f\"http://archivo.asambleanacional.gob.ec{a['href']}\" for a in a_elements]\n",
        "  \n",
        "    return links\n",
        "\n",
        "\n",
        "page_numbers = list(range(1, 4))\n",
        "links = [get_links(page_number) for page_number in page_numbers]\n",
        "urls = [url for sublist in links for url in sublist]\n",
        "\n",
        "for url in urls:\n",
        "    download_PDFs(url)\n"
      ],
      "metadata": {
        "id": "7hW_JZFnWYAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}